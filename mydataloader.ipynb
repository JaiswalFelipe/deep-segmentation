{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mydataloader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Ayc38Qo9yOJM72pp5-7GJHKBmwilKuTm",
      "authorship_tag": "ABX9TyPWw99I7CxB4QJBQ7aN6zKD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaiswalFelipe/deep-segmentation/blob/master/mydataloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import imageio\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from skimage import transform\n",
        "from skimage import img_as_float\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from torchvision import transforms\n",
        "\n",
        "from data_utils import create_or_load_statistics, create_distrib, normalize_images, data_augmentation\n",
        "\n",
        "scaler = MinMaxScaler()"
      ],
      "metadata": {
        "id": "EG8HkKPNQ1iV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NGDataset(Dataset):\n",
        "  def __init__(self, img_dir, mask_dir, img_list, mean, std, transform=None, if_test = False):\n",
        "\n",
        "    self.img_dir = img_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.img_list = img_list\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "    self.transform = transform\n",
        "    self.if_test = if_test\n",
        "    #self.images = os.listdir(img_dir)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_list)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \n",
        "    image = img_as_float(imageio.imread(os.path.join(self.img_dir, self.img_list[index] + '.tif')))\n",
        "    mask = imageio.imread(os.path.join(self.mask_dir, self.img_list[index] + '.tif')).astype(int)\n",
        "\n",
        "    if self.transform is not None:\n",
        "      transformed_img = self.transform(image = image, mask = mask)\n",
        "\n",
        "      image = transformed_img['image']\n",
        "      mask = transformed_img['mask']\n",
        "\n",
        "    #transformed_images = transform.Compose([transform.ToTensor(), transform.Normalize(self.mean, self.std)])\n",
        "\n",
        "    # Normalization.\n",
        "    normalize_images(image, self.mean, self.std) # check data_utils.py\n",
        "    \n",
        "    if self.if_test:\n",
        "      pass \n",
        "    else: \n",
        "      image, mask = data_augmentation(image, mask)\n",
        "     \n",
        "    image = np.transpose(image, (2, 0, 1))\n",
        "\n",
        "    # Turning to tensors.\n",
        "    image = torch.from_numpy(image.copy())\n",
        "    mask = torch.from_numpy(mask.copy())\n",
        "\n",
        "    # Returning to iterator.\n",
        "    return image.float(), mask\n"
      ],
      "metadata": {
        "id": "Q7UNPeWWDVF3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = '/content/drive/MyDrive/training_patches/image_dataset'\n",
        "mask_path = '/content/drive/MyDrive/training_patches/mask_dataset'\n",
        "\n",
        "img_list = []\n",
        "for (_,_,files) in os.walk(img_path):\n",
        "     for file in files:   \n",
        "        img_list.append(file.split('.')[0])\n",
        "        #print (files)\n",
        "        #print ('--------------------------------')"
      ],
      "metadata": {
        "id": "JoBevDAxn688"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = NGDataset(img_path, mask_path, img_list, )"
      ],
      "metadata": {
        "id": "x-R2MNNomgIM"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}